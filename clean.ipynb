{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 880,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT THE LIBRARIES AND OPEN THE CSV FILE AS df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 881,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Case Number', 'Date', 'Year', 'Type', 'Country', 'Area', 'Location',\n",
       "       'Activity', 'Name', 'Sex ', 'Age', 'Injury', 'Fatal (Y/N)', 'Time',\n",
       "       'Species ', 'Investigator or Source', 'pdf', 'href formula', 'href',\n",
       "       'Case Number.1', 'Case Number.2', 'original order', 'Unnamed: 22',\n",
       "       'Unnamed: 23'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 881,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import re\n",
    "from numpy import NaN\n",
    "from pandas import isnull\n",
    "\n",
    "\n",
    "df_raw = pd.read_csv ('./data/attacks.csv', encoding='unicode_escape')\n",
    "\n",
    "# I want to check out the columns that appear in the original dataset (.csv)\n",
    "df_raw.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 882,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''I remove the columns that I don't want -> ['Investigator or Source', 'pdf', 'href formula', 'href',\n",
    "'Case Number.1', 'Case Number.2', 'original order', 'Unnamed: 22','Unnamed: 23'] \n",
    "-> I additionally remove all lines that have null information or more than 2 fields that are null\n",
    "'''\n",
    "\n",
    "df= df_raw.drop(columns=['Investigator or Source', 'pdf', 'href formula', 'href',\n",
    "       'Case Number.1', 'Case Number.2', 'original order', 'Unnamed: 22',\n",
    "       'Unnamed: 23'])\n",
    "df = df.dropna(how=\"all\")\n",
    "df = df.dropna(thresh=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 883,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Case Number     object\n",
       "Date            object\n",
       "Year           float64\n",
       "Type            object\n",
       "Country         object\n",
       "Area            object\n",
       "Location        object\n",
       "Activity        object\n",
       "Name            object\n",
       "Sex             object\n",
       "Age             object\n",
       "Injury          object\n",
       "Fatal (Y/N)     object\n",
       "Time            object\n",
       "Species         object\n",
       "dtype: object"
      ]
     },
     "execution_count": 883,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I want to check the data types of each item\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 884,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Case Number    object\n",
       "Date           object\n",
       "Year            int64\n",
       "Type           object\n",
       "Country        object\n",
       "Area           object\n",
       "Location       object\n",
       "Activity       object\n",
       "Name           object\n",
       "Sex            object\n",
       "Age            object\n",
       "Injury         object\n",
       "Fatal (Y/N)    object\n",
       "Time           object\n",
       "Species        object\n",
       "dtype: object"
      ]
     },
     "execution_count": 884,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I start cleaning the Year Column\n",
    "# Filter - I will keep year > 1900 and will remove data with no Year associated\n",
    "# I change year dtype to INT\n",
    "\n",
    "df.drop (df[df.Year < 1900].index, inplace=True)\n",
    "df = df[df.Year.notna()]\n",
    "df[\"Year\"] = df[\"Year\"].astype(int)\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 885,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['USA', 'AUSTRALIA', 'MEXICO', 'BRAZIL', 'ENGLAND', 'SOUTH AFRICA',\n",
       "       'THAILAND', 'COSTA RICA', 'MALDIVES', 'BAHAMAS', 'NEW CALEDONIA',\n",
       "       'ECUADOR', 'MALAYSIA', 'LIBYA', nan, 'CUBA', 'MAURITIUS',\n",
       "       'NEW ZEALAND', 'SPAIN', 'SAMOA', 'SOLOMON ISLANDS', 'JAPAN',\n",
       "       'EGYPT', 'ST HELENA, BRITISH OVERSEAS TERRITORY', 'COMOROS',\n",
       "       'REUNION', 'FRENCH POLYNESIA', 'UNITED KINGDOM',\n",
       "       'UNITED ARAB EMIRATES', 'PHILIPPINES', 'INDONESIA', 'CHINA',\n",
       "       'COLUMBIA', 'CAPE VERDE', 'FIJI', 'DOMINICAN REPUBLIC',\n",
       "       'CAYMAN ISLANDS', 'ARUBA', 'MOZAMBIQUE', 'PUERTO RICO', 'ITALY',\n",
       "       'ATLANTIC OCEAN', 'GREECE', 'ST. MARTIN', 'FRANCE',\n",
       "       'PAPUA NEW GUINEA', 'TRINIDAD & TOBAGO', 'KIRIBATI', 'ISRAEL',\n",
       "       'DIEGO GARCIA', 'TAIWAN', 'JAMAICA', 'PALESTINIAN TERRITORIES',\n",
       "       'GUAM', 'SEYCHELLES', 'BELIZE', 'NIGERIA', 'TONGA', 'SCOTLAND',\n",
       "       'CANADA', 'CROATIA', 'SAUDI ARABIA', 'CHILE', 'ANTIGUA', 'KENYA',\n",
       "       'RUSSIA', 'TURKS & CAICOS', 'UNITED ARAB EMIRATES (UAE)', 'AZORES',\n",
       "       'SOUTH KOREA', 'MALTA', 'VIETNAM', 'MADAGASCAR', 'PANAMA',\n",
       "       'SOMALIA', 'NEVIS', 'BRITISH VIRGIN ISLANDS', 'NORWAY', 'SENEGAL',\n",
       "       'YEMEN', 'GULF OF ADEN', 'SIERRA LEONE', 'ST. MAARTIN',\n",
       "       'GRAND CAYMAN', 'LIBERIA', 'VANUATU', 'MEXICO ', 'HONDURAS',\n",
       "       'VENEZUELA', 'SRI LANKA', ' TONGA', 'URUGUAY', 'INDIA',\n",
       "       'MICRONESIA', 'CARIBBEAN SEA', 'OKINAWA', 'TANZANIA',\n",
       "       'MARSHALL ISLANDS', 'EGYPT / ISRAEL', 'NORTHERN ARABIAN SEA',\n",
       "       'HONG KONG', 'EL SALVADOR', 'ANGOLA', 'BERMUDA', 'MONTENEGRO',\n",
       "       'IRAN', 'TUNISIA', 'NAMIBIA', 'NORTH ATLANTIC OCEAN', 'PORTUGAL',\n",
       "       'SOUTH CHINA SEA', 'BANGLADESH', 'PALAU', 'WESTERN SAMOA',\n",
       "       'PACIFIC OCEAN ', 'BRITISH ISLES', 'GRENADA', 'IRAQ', 'TURKEY',\n",
       "       'SINGAPORE', 'NEW BRITAIN', 'SUDAN', 'JOHNSTON ISLAND',\n",
       "       'SOUTH PACIFIC OCEAN', 'NEW GUINEA', 'RED SEA',\n",
       "       'NORTH PACIFIC OCEAN', 'FEDERATED STATES OF MICRONESIA',\n",
       "       'MID ATLANTIC OCEAN', 'ADMIRALTY ISLANDS', 'BRITISH WEST INDIES',\n",
       "       'SOUTH ATLANTIC OCEAN', 'PERSIAN GULF', 'RED SEA / INDIAN OCEAN',\n",
       "       'PACIFIC OCEAN', 'NORTH SEA', 'NICARAGUA ', 'MALDIVE ISLANDS',\n",
       "       'AMERICAN SAMOA', 'ANDAMAN / NICOBAR ISLANDAS', 'GABON', 'MAYOTTE',\n",
       "       'NORTH ATLANTIC OCEAN ', 'THE BALKANS', 'SUDAN?', 'ARGENTINA',\n",
       "       'MARTINIQUE', 'INDIAN OCEAN', 'GUATEMALA', 'NETHERLANDS ANTILLES',\n",
       "       'NORTHERN MARIANA ISLANDS', 'IRAN / IRAQ', 'JAVA', ' PHILIPPINES',\n",
       "       'NICARAGUA', 'CENTRAL PACIFIC', 'SOLOMON ISLANDS / VANUATU',\n",
       "       'SOUTHWEST PACIFIC OCEAN', 'BAY OF BENGAL', 'MID-PACIFC OCEAN',\n",
       "       'SLOVENIA', 'CURACAO', 'ICELAND', 'ITALY / CROATIA', 'BARBADOS',\n",
       "       'MONACO', 'GUYANA', 'HAITI', 'SAN DOMINGO', 'IRELAND', 'KUWAIT',\n",
       "       'YEMEN ', 'REUNION ISLAND', 'FALKLAND ISLANDS', 'CRETE', 'CYPRUS'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 885,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I will clean Countries - I will put everything in CAPITAL LETTERS so to remove duplicates with countries\n",
    "\n",
    "df[\"Country\"] = df[\"Country\"].str.upper()\n",
    "df[\"Country\"].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 886,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will clean the Type column - there are 2 blank values and one value that is numeric (val = \"34\")\n",
    "\n",
    "df = df[df.Type.notna()]\n",
    "df.drop (df[df.Type == \"34\"].index, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 887,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Date --> next column\n",
    "\n",
    "#re.search  -> returns True or False\n",
    "\n",
    "#if starts with Reported -> return the rest of the date\n",
    "#I will create a new column --> Month (using regex)\n",
    "#I will create a new column Day --> based on Date column\n",
    "#I will create a new column Season --> based on new column Month\n",
    "\n",
    "#(outcome)  --> [\"Day\"],[\"Month\"],[\"Year\"],[\"Season\"]\n",
    "\n",
    "from random import randint\n",
    "\n",
    "\n",
    "df[\"Date\"] = df[\"Date\"].str.replace(\"Reported \",\"\")\n",
    "\n",
    "df.insert(2,\"Months\",df['Case Number'].str.extract('\\.(\\d{2})\\.'))\n",
    "df.insert(2,\"Day\",df['Case Number'].str.extract('\\.\\d{2}\\.(\\d{2})'))\n",
    "\n",
    "#as I have the data separated by [\"Days\"], [\"Months\"], [\"Year\"] I will remove the \"Date\" column\n",
    "\n",
    "df.drop([\"Date\"], axis=1 ,inplace=True)\n",
    "\n",
    "# I want to turn NaN to \"00\" so that all missing data on date is grouped together -> i will change data type of columns to INT\n",
    "\n",
    "df.loc[df[\"Months\"].isnull()]=\"00\"\n",
    "df.loc[df[\"Day\"].isnull()] =\"00\"\n",
    "\n",
    "\n",
    "df[\"Year\"] = df[\"Year\"].astype(int)\n",
    "df[\"Months\"] = df[\"Months\"].astype(int)\n",
    "df[\"Day\"] = df[\"Day\"].astype(int)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 888,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Case Number    object\n",
      "Day             int64\n",
      "Months          int64\n",
      "Year            int64\n",
      "Type           object\n",
      "Country        object\n",
      "Area           object\n",
      "Location       object\n",
      "Activity       object\n",
      "Name           object\n",
      "Sex            object\n",
      "Age            object\n",
      "Injury         object\n",
      "Fatal (Y/N)    object\n",
      "Time           object\n",
      "Species        object\n",
      "dtype: object\n",
      "[ 6  5  4  3  2  1 12 11 10  9  8  7  0]\n",
      "[25 18  9  8  4  3 27 26 24 21 13  0 12 30 28 23 22 19 15 14 10  5 31 17\n",
      " 11  1 20 16  6  2 29  7]\n"
     ]
    }
   ],
   "source": [
    "print(df.dtypes)\n",
    "print(df[\"Months\"].unique())\n",
    "print(df[\"Day\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 889,
   "metadata": {},
   "outputs": [],
   "source": [
    "south = pd.read_csv ('./data/south.csv', encoding='unicode_escape')\n",
    "south.dtypes\n",
    "# column name --> ï»¿SOUTH COUNTRY \n",
    "\n",
    "list_south = []\n",
    "for elem in south[\"ï»¿SOUTH COUNTRY\"]:\n",
    "    list_south.append(elem)\n",
    "\n",
    "\n",
    "#outcome --> list of southern hemisphere countries\n",
    "\n",
    "#I create column Hemisphere\n",
    "\n",
    "\n",
    "df[\"Hemisphere\"] = np.where(df['Country'].isin(list_south), 'SOUTH', 'NORTH')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 899,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Case Number</th>\n",
       "      <th>Day</th>\n",
       "      <th>Months</th>\n",
       "      <th>Year</th>\n",
       "      <th>Type</th>\n",
       "      <th>Country</th>\n",
       "      <th>Area</th>\n",
       "      <th>Location</th>\n",
       "      <th>Activity</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Injury</th>\n",
       "      <th>Fatal (Y/N)</th>\n",
       "      <th>Time</th>\n",
       "      <th>Species</th>\n",
       "      <th>Hemisphere</th>\n",
       "      <th>Season</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018.06.25</td>\n",
       "      <td>25</td>\n",
       "      <td>6</td>\n",
       "      <td>2018</td>\n",
       "      <td>Boating</td>\n",
       "      <td>USA</td>\n",
       "      <td>California</td>\n",
       "      <td>Oceanside, San Diego County</td>\n",
       "      <td>Paddling</td>\n",
       "      <td>Julie Wolfe</td>\n",
       "      <td>F</td>\n",
       "      <td>57</td>\n",
       "      <td>No injury to occupant, outrigger canoe and pad...</td>\n",
       "      <td>N</td>\n",
       "      <td>18h00</td>\n",
       "      <td>White shark</td>\n",
       "      <td>NORTH</td>\n",
       "      <td>SPRING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018.06.18</td>\n",
       "      <td>18</td>\n",
       "      <td>6</td>\n",
       "      <td>2018</td>\n",
       "      <td>Unprovoked</td>\n",
       "      <td>USA</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>St. Simon Island, Glynn County</td>\n",
       "      <td>Standing</td>\n",
       "      <td>Adyson McNeely</td>\n",
       "      <td>F</td>\n",
       "      <td>11</td>\n",
       "      <td>Minor injury to left thigh</td>\n",
       "      <td>N</td>\n",
       "      <td>14h00  -15h00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NORTH</td>\n",
       "      <td>SPRING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018.06.09</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>2018</td>\n",
       "      <td>Invalid</td>\n",
       "      <td>USA</td>\n",
       "      <td>Hawaii</td>\n",
       "      <td>Habush, Oahu</td>\n",
       "      <td>Surfing</td>\n",
       "      <td>John Denges</td>\n",
       "      <td>M</td>\n",
       "      <td>48</td>\n",
       "      <td>Injury to left lower leg from surfboard skeg</td>\n",
       "      <td>N</td>\n",
       "      <td>07h45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NORTH</td>\n",
       "      <td>SPRING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018.06.08</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>2018</td>\n",
       "      <td>Unprovoked</td>\n",
       "      <td>AUSTRALIA</td>\n",
       "      <td>New South Wales</td>\n",
       "      <td>Arrawarra Headland</td>\n",
       "      <td>Surfing</td>\n",
       "      <td>male</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Minor injury to lower leg</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2 m shark</td>\n",
       "      <td>SOUTH</td>\n",
       "      <td>AUTUMN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018.06.04</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>2018</td>\n",
       "      <td>Provoked</td>\n",
       "      <td>MEXICO</td>\n",
       "      <td>Colima</td>\n",
       "      <td>La Ticla</td>\n",
       "      <td>Free diving</td>\n",
       "      <td>Gustavo Ramos</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Lacerations to leg &amp; hand shark PROVOKED INCIDENT</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tiger shark, 3m</td>\n",
       "      <td>NORTH</td>\n",
       "      <td>SPRING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5559</th>\n",
       "      <td>1900.07.14</td>\n",
       "      <td>14</td>\n",
       "      <td>7</td>\n",
       "      <td>1900</td>\n",
       "      <td>Invalid</td>\n",
       "      <td>USA</td>\n",
       "      <td>Hawaii</td>\n",
       "      <td>Makapu'u Point, O'ahu</td>\n",
       "      <td>Hunting seashells</td>\n",
       "      <td>Emil Uhlbrecht &amp; unidentified person</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Believed drowned. Uhlbrechts foot, and the pe...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Questionable</td>\n",
       "      <td>NORTH</td>\n",
       "      <td>SUMMER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5560</th>\n",
       "      <td>1900.07.00</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1900</td>\n",
       "      <td>Provoked</td>\n",
       "      <td>USA</td>\n",
       "      <td>Connecticut</td>\n",
       "      <td>Bridgeport, Fairfield County</td>\n",
       "      <td></td>\n",
       "      <td>skiff with Dr. William T. Healey, Dr. Henry Ca...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No injury to occupants. They shot shark, then ...</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NORTH</td>\n",
       "      <td>SUMMER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5561</th>\n",
       "      <td>1900.01.28</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>1900</td>\n",
       "      <td>Unprovoked</td>\n",
       "      <td>AUSTRALIA</td>\n",
       "      <td>New South Wales</td>\n",
       "      <td>Lane Cove River, Sydney Harbor (Estuary)</td>\n",
       "      <td>Standing, gathering oysters</td>\n",
       "      <td>Charles Duck</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Right posterior thigh bitten</td>\n",
       "      <td>N</td>\n",
       "      <td>12h00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SOUTH</td>\n",
       "      <td>SUMMER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5562</th>\n",
       "      <td>1900.00.00.b</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1900</td>\n",
       "      <td>Unprovoked</td>\n",
       "      <td>USA</td>\n",
       "      <td>Hawaii</td>\n",
       "      <td>Inter-Island Dry Dock at Kakaako Street, Honol...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Emil A. Berndt</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Severe abrasion when shark swam between his legs</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NORTH</td>\n",
       "      <td>NOT DEFINED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5563</th>\n",
       "      <td>1900.00.00.a</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1900</td>\n",
       "      <td>Unprovoked</td>\n",
       "      <td>SOUTH AFRICA</td>\n",
       "      <td>Eastern Cape Province</td>\n",
       "      <td>Port Elizabeth</td>\n",
       "      <td>Swimming</td>\n",
       "      <td>Mr. Gruner</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Leg severed</td>\n",
       "      <td>N</td>\n",
       "      <td>Early morning</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SOUTH</td>\n",
       "      <td>NOT DEFINED</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5559 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Case Number  Day  Months  Year        Type       Country  \\\n",
       "0       2018.06.25   25       6  2018     Boating           USA   \n",
       "1       2018.06.18   18       6  2018  Unprovoked           USA   \n",
       "2       2018.06.09    9       6  2018     Invalid           USA   \n",
       "3       2018.06.08    8       6  2018  Unprovoked     AUSTRALIA   \n",
       "4       2018.06.04    4       6  2018    Provoked        MEXICO   \n",
       "...            ...  ...     ...   ...         ...           ...   \n",
       "5559    1900.07.14   14       7  1900     Invalid           USA   \n",
       "5560    1900.07.00    0       7  1900    Provoked           USA   \n",
       "5561    1900.01.28   28       1  1900  Unprovoked     AUSTRALIA   \n",
       "5562  1900.00.00.b    0       0  1900  Unprovoked           USA   \n",
       "5563  1900.00.00.a    0       0  1900  Unprovoked  SOUTH AFRICA   \n",
       "\n",
       "                       Area  \\\n",
       "0                California   \n",
       "1                   Georgia   \n",
       "2                    Hawaii   \n",
       "3           New South Wales   \n",
       "4                    Colima   \n",
       "...                     ...   \n",
       "5559                 Hawaii   \n",
       "5560            Connecticut   \n",
       "5561        New South Wales   \n",
       "5562                 Hawaii   \n",
       "5563  Eastern Cape Province   \n",
       "\n",
       "                                               Location  \\\n",
       "0                           Oceanside, San Diego County   \n",
       "1                        St. Simon Island, Glynn County   \n",
       "2                                          Habush, Oahu   \n",
       "3                                    Arrawarra Headland   \n",
       "4                                              La Ticla   \n",
       "...                                                 ...   \n",
       "5559                              Makapu'u Point, O'ahu   \n",
       "5560                       Bridgeport, Fairfield County   \n",
       "5561           Lane Cove River, Sydney Harbor (Estuary)   \n",
       "5562  Inter-Island Dry Dock at Kakaako Street, Honol...   \n",
       "5563                                     Port Elizabeth   \n",
       "\n",
       "                         Activity  \\\n",
       "0                        Paddling   \n",
       "1                        Standing   \n",
       "2                         Surfing   \n",
       "3                         Surfing   \n",
       "4                     Free diving   \n",
       "...                           ...   \n",
       "5559            Hunting seashells   \n",
       "5560                                \n",
       "5561  Standing, gathering oysters   \n",
       "5562                          NaN   \n",
       "5563                     Swimming   \n",
       "\n",
       "                                                   Name Sex   Age  \\\n",
       "0                                           Julie Wolfe    F   57   \n",
       "1                                       Adyson McNeely     F   11   \n",
       "2                                           John Denges    M   48   \n",
       "3                                                  male    M  NaN   \n",
       "4                                        Gustavo Ramos     M  NaN   \n",
       "...                                                 ...  ...  ...   \n",
       "5559               Emil Uhlbrecht & unidentified person    M  NaN   \n",
       "5560  skiff with Dr. William T. Healey, Dr. Henry Ca...  NaN  NaN   \n",
       "5561                                       Charles Duck    M  NaN   \n",
       "5562                                     Emil A. Berndt    M  NaN   \n",
       "5563                                         Mr. Gruner    M  NaN   \n",
       "\n",
       "                                                 Injury Fatal (Y/N)  \\\n",
       "0     No injury to occupant, outrigger canoe and pad...           N   \n",
       "1                            Minor injury to left thigh           N   \n",
       "2          Injury to left lower leg from surfboard skeg           N   \n",
       "3                             Minor injury to lower leg           N   \n",
       "4     Lacerations to leg & hand shark PROVOKED INCIDENT           N   \n",
       "...                                                 ...         ...   \n",
       "5559  Believed drowned. Uhlbrechts foot, and the pe...         NaN   \n",
       "5560  No injury to occupants. They shot shark, then ...           N   \n",
       "5561                       Right posterior thigh bitten           N   \n",
       "5562   Severe abrasion when shark swam between his legs           N   \n",
       "5563                                        Leg severed           N   \n",
       "\n",
       "               Time         Species  Hemisphere       Season  \n",
       "0             18h00      White shark      NORTH       SPRING  \n",
       "1     14h00  -15h00              NaN      NORTH       SPRING  \n",
       "2             07h45              NaN      NORTH       SPRING  \n",
       "3               NaN        2 m shark      SOUTH       AUTUMN  \n",
       "4               NaN  Tiger shark, 3m      NORTH       SPRING  \n",
       "...             ...              ...        ...          ...  \n",
       "5559            NaN     Questionable      NORTH       SUMMER  \n",
       "5560            NaN              NaN      NORTH       SUMMER  \n",
       "5561          12h00              NaN      SOUTH       SUMMER  \n",
       "5562            NaN              NaN      NORTH  NOT DEFINED  \n",
       "5563  Early morning              NaN      SOUTH  NOT DEFINED  \n",
       "\n",
       "[5559 rows x 18 columns]"
      ]
     },
     "execution_count": 899,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I will create the season column (based on the hemisphere and the month)\n",
    "\n",
    "season_list = []\n",
    "list_hemis = list(df[\"Hemisphere\"])\n",
    "list_month = list(df[\"Months\"])\n",
    "\n",
    "for h,m in zip(list_hemis,list_month):\n",
    "    if h == \"NORTH\":\n",
    "        if m in range(1,4):\n",
    "            season_list.append(\"WINTER\")\n",
    "        elif m in range(4,7):\n",
    "            season_list.append(\"SPRING\")\n",
    "        elif m in range(7,10):\n",
    "            season_list.append(\"SUMMER\")\n",
    "        elif m in range(10,13):\n",
    "            season_list.append(\"AUTUMN\")\n",
    "        else: \n",
    "            season_list.append(\"NOT DEFINED\")\n",
    "\n",
    "    elif h == \"SOUTH\":\n",
    "        if m in range(1,4):\n",
    "            season_list.append(\"SUMMER\")\n",
    "        elif m in range(4,7):\n",
    "            season_list.append(\"AUTUMN\")\n",
    "        elif m in range(7,10):\n",
    "            season_list.append(\"WINTER\")\n",
    "        elif m in range(10,13):\n",
    "            season_list.append(\"SPRING\")\n",
    "        else: \n",
    "            season_list.append(\"NOT DEFINED\")\n",
    "\n",
    "df[\"Season\"] = season_list\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 892,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'filterinfDataframe = dfObj[(dfObj[\\'Sale\\'] > 30) & (dfObj[\\'Sale\\'] < 33)\\ndf1 = df\\ndf1[\"Season\"] = df1[\"SPRING\"if (df1[\"Hemisphere\"] == \"NORTH\" & (df1[\"Months\"].between)]\\n\\ndf[\"Season\"] = [\"SPRING\" if x == \"NORTH\" and y in range(4,7) else \"tbd\" for x,y in zip(df[\"Hemisphere\"],df[\"Months\"])]\\ndf[\"Season\"] = [\"SUMMER\" if x == \"NORTH\" and y in range(7,10) else \"tbd\" for x,y in zip(df[\"Hemisphere\"],df[\"Months\"])]\\ndf[\"Season\"] = [\"WINTER\" if x == \"NORTH\" and y in range(10,13) else \"tbd\" for x,y in zip(df[\"Hemisphere\"],df[\"Months\"])]\\ndf'"
      ]
     },
     "execution_count": 892,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''filterinfDataframe = dfObj[(dfObj['Sale'] > 30) & (dfObj['Sale'] < 33)\n",
    "df1 = df\n",
    "df1[\"Season\"] = df1[\"SPRING\"if (df1[\"Hemisphere\"] == \"NORTH\" & (df1[\"Months\"].between)]\n",
    "\n",
    "df[\"Season\"] = [\"SPRING\" if x == \"NORTH\" and y in range(4,7) else \"tbd\" for x,y in zip(df[\"Hemisphere\"],df[\"Months\"])]\n",
    "df[\"Season\"] = [\"SUMMER\" if x == \"NORTH\" and y in range(7,10) else \"tbd\" for x,y in zip(df[\"Hemisphere\"],df[\"Months\"])]\n",
    "df[\"Season\"] = [\"WINTER\" if x == \"NORTH\" and y in range(10,13) else \"tbd\" for x,y in zip(df[\"Hemisphere\"],df[\"Months\"])]\n",
    "df'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 893,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef season (df):\\n    if df.loc[\"Hemisphere\"].str.contains(\"NORTH\"):\\n        if df[\"Months\"] in (range(1,4)):\\n            return \"WINTER\"\\n        elif df[\"Months\"]  in (range(4,7)):\\n            return \"SPRING\"\\n        elif df[\"Months\"] in (range(7,10)):\\n            return \"SUMMER\"\\n        elif df[\"Months\"] in (range(10,13)):\\n            return \"AUTUMN\"\\n        elif df[\"Months\"] == 0:\\n            return str(NaN)\\n    elif df.loc[\"Hemisphere\"].str.contains (\"SOUTH\"):\\n        if df[\"Months\"] in (range(1,4)):\\n            return \"SUMMER\"\\n        elif df[\"Months\"] in (range(4,7)):\\n            return \"AUTUMN\"\\n        elif df[\"Months\"] in (range(7,10)):\\n            return \"WINTER\"\\n        elif df[\"Months\"] in (range(10,13)):\\n            return \"SPRING\"\\n        elif df[\"Months\"] == 0:\\n            return str(NaN)\\n\\n    elif df[\"Hemisphere\"].isnull():\\n        return str(NaN)\\n\\nseason(df)\\n\\n'"
      ]
     },
     "execution_count": 893,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def season (df):\n",
    "    if df.loc[\"Hemisphere\"].str.contains(\"NORTH\"):\n",
    "        if df[\"Months\"] in (range(1,4)):\n",
    "            return \"WINTER\"\n",
    "        elif df[\"Months\"]  in (range(4,7)):\n",
    "            return \"SPRING\"\n",
    "        elif df[\"Months\"] in (range(7,10)):\n",
    "            return \"SUMMER\"\n",
    "        elif df[\"Months\"] in (range(10,13)):\n",
    "            return \"AUTUMN\"\n",
    "        elif df[\"Months\"] == 0:\n",
    "            return str(NaN)\n",
    "    elif df.loc[\"Hemisphere\"].str.contains (\"SOUTH\"):\n",
    "        if df[\"Months\"] in (range(1,4)):\n",
    "            return \"SUMMER\"\n",
    "        elif df[\"Months\"] in (range(4,7)):\n",
    "            return \"AUTUMN\"\n",
    "        elif df[\"Months\"] in (range(7,10)):\n",
    "            return \"WINTER\"\n",
    "        elif df[\"Months\"] in (range(10,13)):\n",
    "            return \"SPRING\"\n",
    "        elif df[\"Months\"] == 0:\n",
    "            return str(NaN)\n",
    "\n",
    "    elif df[\"Hemisphere\"].isnull():\n",
    "        return str(NaN)\n",
    "\n",
    "season(df)\n",
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('ironhack')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f675732d597f42bb0438ce2c5e2877b55acd9e5ba267d3e24f04094c82602a22"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
