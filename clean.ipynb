{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2051,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT THE LIBRARIES AND OPEN THE CSV FILE AS df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2052,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Case Number', 'Date', 'Year', 'Type', 'Country', 'Area', 'Location',\n",
       "       'Activity', 'Name', 'Sex ', 'Age', 'Injury', 'Fatal (Y/N)', 'Time',\n",
       "       'Species ', 'Investigator or Source', 'pdf', 'href formula', 'href',\n",
       "       'Case Number.1', 'Case Number.2', 'original order', 'Unnamed: 22',\n",
       "       'Unnamed: 23'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 2052,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import re\n",
    "from numpy import NaN\n",
    "from pandas import isnull\n",
    "\n",
    "\n",
    "df_raw = pd.read_csv ('./data/attacks.csv', encoding='unicode_escape')\n",
    "\n",
    "# I want to check out the columns that appear in the original dataset (.csv)\n",
    "df_raw.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2053,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Case Number', 'Date', 'Year', 'Type', 'Country', 'Area', 'Location',\n",
       "       'Activity', 'Name', 'Sex', 'Age', 'Fatal (Y/N)', 'Time', 'Species'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 2053,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''I remove the columns that I don't want -> ['Investigator or Source', 'pdf', 'href formula', 'href',\n",
    "'Case Number.1', 'Case Number.2', 'original order', 'Unnamed: 22','Unnamed: 23'] \n",
    "-> I additionally remove all lines that have null information or more than 2 fields that are null\n",
    "'''\n",
    "\n",
    "df= df_raw.drop(columns=['Injury','Investigator or Source', 'pdf', 'href formula', 'href',\n",
    "       'Case Number.1', 'Case Number.2', 'original order', 'Unnamed: 22',\n",
    "       'Unnamed: 23'])\n",
    "df = df.dropna(how=\"all\")\n",
    "df = df.dropna(thresh=3)\n",
    "\n",
    "df.rename(columns = {\"Species \":\"Species\"}, inplace=True)\n",
    "df.rename(columns = {\"Sex \":\"Sex\"}, inplace=True)\n",
    "df.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2054,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Case Number     object\n",
       "Date            object\n",
       "Year           float64\n",
       "Type            object\n",
       "Country         object\n",
       "Area            object\n",
       "Location        object\n",
       "Activity        object\n",
       "Name            object\n",
       "Sex             object\n",
       "Age             object\n",
       "Fatal (Y/N)     object\n",
       "Time            object\n",
       "Species         object\n",
       "dtype: object"
      ]
     },
     "execution_count": 2054,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I want to check the data types of each item\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2055,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Case Number    object\n",
       "Date           object\n",
       "Year            int64\n",
       "Type           object\n",
       "Country        object\n",
       "Area           object\n",
       "Location       object\n",
       "Activity       object\n",
       "Name           object\n",
       "Sex            object\n",
       "Age            object\n",
       "Fatal (Y/N)    object\n",
       "Time           object\n",
       "Species        object\n",
       "dtype: object"
      ]
     },
     "execution_count": 2055,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I start cleaning the Year Column\n",
    "# Filter - I will keep year > 1900 and will remove data with no Year associated\n",
    "# I change year dtype to INT\n",
    "\n",
    "df.drop (df[df.Year < 1900].index, inplace=True)\n",
    "df = df[df.Year.notna()]\n",
    "df[\"Year\"] = df[\"Year\"].astype(int)\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2056,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['USA', 'AUSTRALIA', 'MEXICO', 'BRAZIL', 'ENGLAND', 'SOUTH AFRICA',\n",
       "       'THAILAND', 'COSTA RICA', 'MALDIVES', 'BAHAMAS', 'NEW CALEDONIA',\n",
       "       'ECUADOR', 'MALAYSIA', 'LIBYA', nan, 'CUBA', 'MAURITIUS',\n",
       "       'NEW ZEALAND', 'SPAIN', 'SAMOA', 'SOLOMON ISLANDS', 'JAPAN',\n",
       "       'EGYPT', 'ST HELENA, BRITISH OVERSEAS TERRITORY', 'COMOROS',\n",
       "       'REUNION', 'FRENCH POLYNESIA', 'UNITED KINGDOM',\n",
       "       'UNITED ARAB EMIRATES', 'PHILIPPINES', 'INDONESIA', 'CHINA',\n",
       "       'COLUMBIA', 'CAPE VERDE', 'FIJI', 'DOMINICAN REPUBLIC',\n",
       "       'CAYMAN ISLANDS', 'ARUBA', 'MOZAMBIQUE', 'PUERTO RICO', 'ITALY',\n",
       "       'ATLANTIC OCEAN', 'GREECE', 'ST. MARTIN', 'FRANCE',\n",
       "       'PAPUA NEW GUINEA', 'TRINIDAD & TOBAGO', 'KIRIBATI', 'ISRAEL',\n",
       "       'DIEGO GARCIA', 'TAIWAN', 'JAMAICA', 'PALESTINIAN TERRITORIES',\n",
       "       'GUAM', 'SEYCHELLES', 'BELIZE', 'NIGERIA', 'TONGA', 'SCOTLAND',\n",
       "       'CANADA', 'CROATIA', 'SAUDI ARABIA', 'CHILE', 'ANTIGUA', 'KENYA',\n",
       "       'RUSSIA', 'TURKS & CAICOS', 'UNITED ARAB EMIRATES (UAE)', 'AZORES',\n",
       "       'SOUTH KOREA', 'MALTA', 'VIETNAM', 'MADAGASCAR', 'PANAMA',\n",
       "       'SOMALIA', 'NEVIS', 'BRITISH VIRGIN ISLANDS', 'NORWAY', 'SENEGAL',\n",
       "       'YEMEN', 'GULF OF ADEN', 'SIERRA LEONE', 'ST. MAARTIN',\n",
       "       'GRAND CAYMAN', 'LIBERIA', 'VANUATU', 'MEXICO ', 'HONDURAS',\n",
       "       'VENEZUELA', 'SRI LANKA', ' TONGA', 'URUGUAY', 'INDIA',\n",
       "       'MICRONESIA', 'CARIBBEAN SEA', 'OKINAWA', 'TANZANIA',\n",
       "       'MARSHALL ISLANDS', 'EGYPT / ISRAEL', 'NORTHERN ARABIAN SEA',\n",
       "       'HONG KONG', 'EL SALVADOR', 'ANGOLA', 'BERMUDA', 'MONTENEGRO',\n",
       "       'IRAN', 'TUNISIA', 'NAMIBIA', 'NORTH ATLANTIC OCEAN', 'PORTUGAL',\n",
       "       'SOUTH CHINA SEA', 'BANGLADESH', 'PALAU', 'WESTERN SAMOA',\n",
       "       'PACIFIC OCEAN ', 'BRITISH ISLES', 'GRENADA', 'IRAQ', 'TURKEY',\n",
       "       'SINGAPORE', 'NEW BRITAIN', 'SUDAN', 'JOHNSTON ISLAND',\n",
       "       'SOUTH PACIFIC OCEAN', 'NEW GUINEA', 'RED SEA',\n",
       "       'NORTH PACIFIC OCEAN', 'FEDERATED STATES OF MICRONESIA',\n",
       "       'MID ATLANTIC OCEAN', 'ADMIRALTY ISLANDS', 'BRITISH WEST INDIES',\n",
       "       'SOUTH ATLANTIC OCEAN', 'PERSIAN GULF', 'RED SEA / INDIAN OCEAN',\n",
       "       'PACIFIC OCEAN', 'NORTH SEA', 'NICARAGUA ', 'MALDIVE ISLANDS',\n",
       "       'AMERICAN SAMOA', 'ANDAMAN / NICOBAR ISLANDAS', 'GABON', 'MAYOTTE',\n",
       "       'NORTH ATLANTIC OCEAN ', 'THE BALKANS', 'SUDAN?', 'ARGENTINA',\n",
       "       'MARTINIQUE', 'INDIAN OCEAN', 'GUATEMALA', 'NETHERLANDS ANTILLES',\n",
       "       'NORTHERN MARIANA ISLANDS', 'IRAN / IRAQ', 'JAVA', ' PHILIPPINES',\n",
       "       'NICARAGUA', 'CENTRAL PACIFIC', 'SOLOMON ISLANDS / VANUATU',\n",
       "       'SOUTHWEST PACIFIC OCEAN', 'BAY OF BENGAL', 'MID-PACIFC OCEAN',\n",
       "       'SLOVENIA', 'CURACAO', 'ICELAND', 'ITALY / CROATIA', 'BARBADOS',\n",
       "       'MONACO', 'GUYANA', 'HAITI', 'SAN DOMINGO', 'IRELAND', 'KUWAIT',\n",
       "       'YEMEN ', 'REUNION ISLAND', 'FALKLAND ISLANDS', 'CRETE', 'CYPRUS'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 2056,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I will clean Countries - I will put everything in CAPITAL LETTERS so to remove duplicates with countries\n",
    "\n",
    "df[\"Country\"] = df[\"Country\"].str.upper()\n",
    "df[\"Country\"].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2057,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will clean the Type column - there are 2 blank values and one value that is numeric (val = \"34\")\n",
    "\n",
    "df = df[df.Type.notna()]\n",
    "df.drop (df[df.Type == \"34\"].index, inplace=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2058,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Date --> next column\n",
    "\n",
    "#re.search  -> returns True or False\n",
    "\n",
    "#if starts with Reported -> return the rest of the date\n",
    "#I will create a new column --> Month (using regex)\n",
    "#I will create a new column Day --> based on Date column\n",
    "#I will create a new column Season --> based on new column Month\n",
    "\n",
    "#(outcome)  --> [\"Day\"],[\"Month\"],[\"Year\"],[\"Season\"]\n",
    "\n",
    "from random import randint\n",
    "\n",
    "\n",
    "df[\"Date\"] = df[\"Date\"].str.replace(\"Reported \",\"\")\n",
    "\n",
    "df.insert(2,\"Months\",df['Case Number'].str.extract('\\.(\\d{2})\\.'))\n",
    "df.insert(2,\"Day\",df['Case Number'].str.extract('\\.\\d{2}\\.(\\d{2})'))\n",
    "\n",
    "#as I have the data separated by [\"Days\"], [\"Months\"], [\"Year\"] I will remove the \"Date\" column\n",
    "\n",
    "df.drop([\"Date\"], axis=1 ,inplace=True)\n",
    "\n",
    "# I want to turn NaN to \"00\" so that all missing data on date is grouped together -> i will change data type of columns to INT\n",
    "\n",
    "df.loc[df[\"Months\"].isnull()]=\"00\"\n",
    "df.loc[df[\"Day\"].isnull()] =\"00\"\n",
    "\n",
    "\n",
    "\n",
    "df[\"Year\"] = df[\"Year\"].astype(int)\n",
    "df[\"Months\"] = df[\"Months\"].astype(int)\n",
    "df[\"Day\"] = df[\"Day\"].astype(int)\n",
    "\n",
    "df = df.drop(columns=[\"Case Number\", \"Time\"])\n",
    "#I remove columns I will not use: Case Number and Time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2059,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Day             int64\n",
      "Months          int64\n",
      "Year            int64\n",
      "Type           object\n",
      "Country        object\n",
      "Area           object\n",
      "Location       object\n",
      "Activity       object\n",
      "Name           object\n",
      "Sex            object\n",
      "Age            object\n",
      "Fatal (Y/N)    object\n",
      "Species        object\n",
      "dtype: object\n",
      "[ 6  5  4  3  2  1 12 11 10  9  8  7  0]\n",
      "[25 18  9  8  4  3 27 26 24 21 13  0 12 30 28 23 22 19 15 14 10  5 31 17\n",
      " 11  1 20 16  6  2 29  7]\n"
     ]
    }
   ],
   "source": [
    "print(df.dtypes)\n",
    "print(df[\"Months\"].unique())\n",
    "print(df[\"Day\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2060,
   "metadata": {},
   "outputs": [],
   "source": [
    "south = pd.read_csv ('./data/south.csv', encoding='unicode_escape')\n",
    "south.dtypes\n",
    "# column name --> ï»¿SOUTH COUNTRY \n",
    "\n",
    "list_south = []\n",
    "for elem in south[\"ï»¿SOUTH COUNTRY\"]:\n",
    "    list_south.append(elem)\n",
    "\n",
    "\n",
    "#outcome --> list of southern hemisphere countries\n",
    "\n",
    "#I create column Hemisphere\n",
    "\n",
    "\n",
    "df[\"Hemisphere\"] = np.where(df['Country'].isin(list_south), 'SOUTH', 'NORTH')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2061,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Day</th>\n",
       "      <th>Months</th>\n",
       "      <th>Year</th>\n",
       "      <th>Season</th>\n",
       "      <th>Type</th>\n",
       "      <th>Hemisphere</th>\n",
       "      <th>Country</th>\n",
       "      <th>Area</th>\n",
       "      <th>Location</th>\n",
       "      <th>Activity</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Fatal (Y/N)</th>\n",
       "      <th>Species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25</td>\n",
       "      <td>6</td>\n",
       "      <td>2018</td>\n",
       "      <td>SPRING</td>\n",
       "      <td>Boating</td>\n",
       "      <td>NORTH</td>\n",
       "      <td>USA</td>\n",
       "      <td>California</td>\n",
       "      <td>Oceanside, San Diego County</td>\n",
       "      <td>Paddling</td>\n",
       "      <td>Julie Wolfe</td>\n",
       "      <td>F</td>\n",
       "      <td>57</td>\n",
       "      <td>N</td>\n",
       "      <td>White shark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18</td>\n",
       "      <td>6</td>\n",
       "      <td>2018</td>\n",
       "      <td>SPRING</td>\n",
       "      <td>Unprovoked</td>\n",
       "      <td>NORTH</td>\n",
       "      <td>USA</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>St. Simon Island, Glynn County</td>\n",
       "      <td>Standing</td>\n",
       "      <td>Adyson McNeely</td>\n",
       "      <td>F</td>\n",
       "      <td>11</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>2018</td>\n",
       "      <td>SPRING</td>\n",
       "      <td>Invalid</td>\n",
       "      <td>NORTH</td>\n",
       "      <td>USA</td>\n",
       "      <td>Hawaii</td>\n",
       "      <td>Habush, Oahu</td>\n",
       "      <td>Surfing</td>\n",
       "      <td>John Denges</td>\n",
       "      <td>M</td>\n",
       "      <td>48</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>2018</td>\n",
       "      <td>AUTUMN</td>\n",
       "      <td>Unprovoked</td>\n",
       "      <td>SOUTH</td>\n",
       "      <td>AUSTRALIA</td>\n",
       "      <td>New South Wales</td>\n",
       "      <td>Arrawarra Headland</td>\n",
       "      <td>Surfing</td>\n",
       "      <td>male</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>N</td>\n",
       "      <td>2 m shark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>2018</td>\n",
       "      <td>SPRING</td>\n",
       "      <td>Provoked</td>\n",
       "      <td>NORTH</td>\n",
       "      <td>MEXICO</td>\n",
       "      <td>Colima</td>\n",
       "      <td>La Ticla</td>\n",
       "      <td>Free diving</td>\n",
       "      <td>Gustavo Ramos</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>N</td>\n",
       "      <td>Tiger shark, 3m</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Day  Months  Year  Season        Type Hemisphere    Country  \\\n",
       "0   25       6  2018  SPRING     Boating      NORTH        USA   \n",
       "1   18       6  2018  SPRING  Unprovoked      NORTH        USA   \n",
       "2    9       6  2018  SPRING     Invalid      NORTH        USA   \n",
       "3    8       6  2018  AUTUMN  Unprovoked      SOUTH  AUSTRALIA   \n",
       "4    4       6  2018  SPRING    Provoked      NORTH     MEXICO   \n",
       "\n",
       "              Area                        Location     Activity  \\\n",
       "0       California     Oceanside, San Diego County     Paddling   \n",
       "1          Georgia  St. Simon Island, Glynn County     Standing   \n",
       "2           Hawaii                    Habush, Oahu      Surfing   \n",
       "3  New South Wales              Arrawarra Headland      Surfing   \n",
       "4           Colima                        La Ticla  Free diving   \n",
       "\n",
       "              Name Sex  Age Fatal (Y/N)          Species  \n",
       "0      Julie Wolfe   F   57           N      White shark  \n",
       "1  Adyson McNeely    F   11           N              NaN  \n",
       "2      John Denges   M   48           N              NaN  \n",
       "3             male   M  NaN           N        2 m shark  \n",
       "4   Gustavo Ramos    M  NaN           N  Tiger shark, 3m  "
      ]
     },
     "execution_count": 2061,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I will create the season column (based on the hemisphere and the month)\n",
    "\n",
    "season_list = []\n",
    "list_hemis = list(df[\"Hemisphere\"])\n",
    "list_month = list(df[\"Months\"])\n",
    "\n",
    "for h,m in zip(list_hemis,list_month):\n",
    "    if h == \"NORTH\":\n",
    "        if m in range(1,4):\n",
    "            season_list.append(\"WINTER\")\n",
    "        elif m in range(4,7):\n",
    "            season_list.append(\"SPRING\")\n",
    "        elif m in range(7,10):\n",
    "            season_list.append(\"SUMMER\")\n",
    "        elif m in range(10,13):\n",
    "            season_list.append(\"AUTUMN\")\n",
    "        else: \n",
    "            season_list.append(\"NOT DEFINED\")\n",
    "\n",
    "    elif h == \"SOUTH\":\n",
    "        if m in range(1,4):\n",
    "            season_list.append(\"SUMMER\")\n",
    "        elif m in range(4,7):\n",
    "            season_list.append(\"AUTUMN\")\n",
    "        elif m in range(7,10):\n",
    "            season_list.append(\"WINTER\")\n",
    "        elif m in range(10,13):\n",
    "            season_list.append(\"SPRING\")\n",
    "        else: \n",
    "            season_list.append(\"NOT DEFINED\")\n",
    "\n",
    "df[\"Season\"] = season_list\n",
    "\n",
    "#i will change the order of teh columns!\n",
    "\n",
    "df= df.reindex(columns=['Day', 'Months', 'Year','Season', 'Type', 'Hemisphere','Country', 'Area','Location', 'Activity', 'Name', 'Sex', 'Age', 'Fatal (Y/N)','Species'])\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2062,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Day             int64\n",
       "Months          int64\n",
       "Year            int64\n",
       "Season         object\n",
       "Type           object\n",
       "Hemisphere     object\n",
       "Country        object\n",
       "Area           object\n",
       "Location       object\n",
       "Activity       object\n",
       "Name           object\n",
       "Sex            object\n",
       "Age            object\n",
       "Fatal (Y/N)    object\n",
       "Species        object\n",
       "dtype: object"
      ]
     },
     "execution_count": 2062,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2063,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['F', 'M', nan], dtype=object)"
      ]
     },
     "execution_count": 2063,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I clean Sex column\n",
    "\n",
    "df[\"Sex\"] = df[\"Sex\"].str.extract(\"(^M|F)\")\n",
    "df[\"Sex\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2064,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['N', 'Y', nan], dtype=object)"
      ]
     },
     "execution_count": 2064,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I clean Fatal column\n",
    "df[\"Fatal (Y/N)\"] = df[\"Fatal (Y/N)\"].str.extract(\"(^Y|N)\")\n",
    "df[\"Fatal (Y/N)\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2065,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['57', '11', '48', nan, '18', '52', '15', '12', '32', '10', '21',\n",
       "       '34', '30', '60', '33', '29', '54', '41', '37', '56', '19', '25',\n",
       "       '69', '38', '55', '35', '46', '45', '14', '40', '28', '20', '24',\n",
       "       '26', '49', '22', '7', '31', '17', '13', '42', '3', '8', '50',\n",
       "       '16', '82', '73', '68', '51', '39', '58', '47', '61', '65', '36',\n",
       "       '66', '43', '9', '72', '59', '6', '27', '64', '23', '71', '44',\n",
       "       '62', '00', '63', '70', '53', '77', '74', '5', '86', '84', '75',\n",
       "       '87', '67', '1', '2', '81', '78'], dtype=object)"
      ]
     },
     "execution_count": 2065,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I will proceed with the cleaning of the age\n",
    "df[\"Age\"] = df[\"Age\"].str.extract(\"(\\d{1,2})\")\n",
    "df[\"Age\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2066,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's clean species!!\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df[\"Species\"] = df[\"Species\"].str.extract(\"([A-Z|a-z]{1,}\\s{1}shark)\")\n",
    "df[\"Species\"] = df[\"Species\"].str.upper()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2067,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['WHITE SHARK', nan, 'M SHARK', 'TIGER SHARK', 'LEMON SHARK',\n",
       "       'BULL SHARK', 'REEF SHARK', 'NURSE SHARK', 'WOBBEGONG SHARK',\n",
       "       'BLACKTIP SHARK', 'A SHARK', 'GALAPAGOS SHARK', 'SMALL SHARK',\n",
       "       'BLUE SHARK', 'COOKIECUTTER SHARK', 'SPINNER SHARK',\n",
       "       'WHITETIP SHARK', 'SANDTIGER SHARK', 'NO SHARK', 'GILL SHARK',\n",
       "       'SEVENGILL SHARK', 'ANGEL SHARK', 'DOGFISH SHARK', 'MAKO SHARK',\n",
       "       'WHALER SHARK', 'SILKY SHARK', 'JUVENILE SHARK',\n",
       "       'HAMMERHEAD SHARK', 'BUT SHARK', 'FOOT SHARK', 'RAGGEDTOOTH SHARK',\n",
       "       'GOBLIN SHARK', 'METRE SHARK', 'SANDBAR SHARK', 'COW SHARK',\n",
       "       'SALMON SHARK', 'PORBEAGLE SHARK', 'JACKSON SHARK',\n",
       "       'ZAMBESI SHARK', 'KG SHARK', 'THRESHER SHARK', 'WHALE SHARK',\n",
       "       'CUTTER SHARK', 'DUSKY SHARK', 'SMOOTHHOUND SHARK',\n",
       "       'BASKING SHARK', 'AS SHARK', 'SAND SHARK', 'SAME SHARK',\n",
       "       'COPPER SHARK', 'BROWN SHARK', 'COLORED SHARK', 'CAPTIVE SHARK',\n",
       "       'BONNETHED SHARK', 'FINNED SHARK', 'SOUPFIN SHARK', 'YOUNG SHARK',\n",
       "       'LEOPARD SHARK', 'UNIDENTIFIED SHARK', 'GREY SHARK',\n",
       "       'FEMALE SHARK', 'TWO SHARK', 'LB SHARK', 'GAFFED SHARK',\n",
       "       'SILVERTIP SHARK', 'ZAMBEZI SHARK', 'GRAY SHARK', 'CARPET SHARK',\n",
       "       'DOG SHARK', 'FOR SHARK', 'RED SHARK', 'BANJO SHARK',\n",
       "       'HOOKED SHARK', 'LARGER SHARK', 'SEVERAL SHARK',\n",
       "       'CARCHARINID SHARK', 'ANOTHER SHARK', 'OF SHARK', 'LITTLE SHARK',\n",
       "       'BONITA SHARK', 'LARGE SHARK', 'SHOVELNOSE SHARK', 'NOSE SHARK',\n",
       "       'NOSED SHARK', 'COCKTAIL SHARK', 'CARCHARHINID SHARK',\n",
       "       'WHIPTAIL SHARK', 'THE SHARK', 'SAW SHARK', 'FROM SHARK'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 2067,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Species\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2068,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Surfing', 'Other', 'Diving', 'Swimming', 'Fishing', 'Walking',\n",
       "       'Sailing'], dtype=object)"
      ]
     },
     "execution_count": 2068,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I will clean activities now!!!\n",
    "new_activity = []\n",
    "# I will group activities within the following groups [\"Diving\", \"Surfing\", \"Swimming\", \"Fishing\",\"Walking\", \"Sailing\",\"Other\"]\n",
    "df[\"Activity\"]=df[\"Activity\"].str.upper()\n",
    "activity= df[\"Activity\"]\n",
    "\n",
    "for word in activity:\n",
    "    word=str(word)\n",
    "    if \"SURF\" in word:\n",
    "        new_activity.append(\"Surfing\")\n",
    "    \n",
    "    elif \"SURFING\" in word:\n",
    "        new_activity.append(\"Surfing\")\n",
    "    \n",
    "    elif \"PADDL\" in word:\n",
    "        new_activity.append(\"Surfing\")\n",
    "\n",
    "    elif \"DIVE\" in word:\n",
    "        new_activity.append(\"Diving\")\n",
    "    \n",
    "    elif \"DIVING\" in word:\n",
    "        new_activity.append(\"Diving\")\n",
    "\n",
    "    elif \"SWIM\" in word:\n",
    "        new_activity.append(\"Swimming\")\n",
    "    \n",
    "    elif \"FLOAT\" in word:\n",
    "        new_activity.append(\"Swimming\")\n",
    "    \n",
    "    elif \"FISH\" in word:\n",
    "        new_activity.append(\"Fishing\")\n",
    "    \n",
    "    elif \"NETS\" in word:\n",
    "        new_activity.append(\"Fishing\")\n",
    "\n",
    "    elif \"WALK\" in word:\n",
    "        new_activity.append(\"Walking\")\n",
    "\n",
    "    elif \"BOAT\" in word:\n",
    "        new_activity.append(\"Sailing\")\n",
    "\n",
    "    else:\n",
    "        new_activity.append(\"Other\")\n",
    "        \n",
    "\n",
    "df[\"Activity\"] = new_activity\n",
    "df[\"Activity\"].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2069,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6,  5,  4,  3,  2,  1, 12, 11, 10,  9,  8,  7])"
      ]
     },
     "execution_count": 2069,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "###NO M'ELIMINA ELS PUTUS VALORS RAROS!!! -> AL CSV S'EM QUEDEN\n",
    "\n",
    "df = df.drop (df[df.Type == \"00\"].index)\n",
    "df = df.drop (df[df.Day == \"F\"].index)\n",
    "df= df.drop (df[df.Months == 0].index)\n",
    "#df.drop (df[df.Day == \"Fishing\"].index, inplace=True)\n",
    "df[\"Months\"].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2070,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"sharkprova.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('ironhack')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f675732d597f42bb0438ce2c5e2877b55acd9e5ba267d3e24f04094c82602a22"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
